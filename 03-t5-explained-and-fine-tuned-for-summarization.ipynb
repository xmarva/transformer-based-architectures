{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3140615,"sourceType":"datasetVersion","datasetId":1912571}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate nltk rouge_score py7zr\n!pip install accelerate\n!pip install sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:56:28.117989Z","iopub.execute_input":"2025-04-23T23:56:28.118256Z","iopub.status.idle":"2025-04-23T23:57:48.964979Z","shell.execute_reply.started":"2025-04-23T23:56:28.118230Z","shell.execute_reply":"2025-04-23T23:57:48.964228Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting py7zr\n  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.22.0)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting brotli>=1.1.0 (from py7zr)\n  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (7.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=445892f1dde72464bef828c33acec12634a32b5c6033825eb76c84c83d9cfe55\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: brotli, pyzstd, pyppmd, pybcj, multivolumefile, inflate64, fsspec, py7zr, rouge_score, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed brotli-1.1.0 evaluate-0.4.3 fsspec-2024.12.0 inflate64-1.0.1 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.3 pyppmd-1.1.1 pyzstd-0.16.2 rouge_score-0.1.2\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    get_scheduler,\n    set_seed\n)\nfrom datasets import load_dataset\nimport evaluate\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport wandb\nimport gc\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:05.604407Z","iopub.execute_input":"2025-04-23T23:58:05.604680Z","iopub.status.idle":"2025-04-23T23:58:37.866291Z","shell.execute_reply.started":"2025-04-23T23:58:05.604659Z","shell.execute_reply":"2025-04-23T23:58:37.865533Z"}},"outputs":[{"name":"stderr","text":"2025-04-23 23:58:21.415138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745452701.595280      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745452701.649534      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:37.867605Z","iopub.execute_input":"2025-04-23T23:58:37.867874Z","iopub.status.idle":"2025-04-23T23:58:37.965715Z","shell.execute_reply.started":"2025-04-23T23:58:37.867849Z","shell.execute_reply":"2025-04-23T23:58:37.965144Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"SEED = 42\nset_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:43.003112Z","iopub.execute_input":"2025-04-23T23:58:43.003764Z","iopub.status.idle":"2025-04-23T23:58:43.011976Z","shell.execute_reply.started":"2025-04-23T23:58:43.003737Z","shell.execute_reply":"2025-04-23T23:58:43.011014Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:44.992083Z","iopub.execute_input":"2025-04-23T23:58:44.992824Z","iopub.status.idle":"2025-04-23T23:58:44.996876Z","shell.execute_reply.started":"2025-04-23T23:58:44.992798Z","shell.execute_reply":"2025-04-23T23:58:44.996250Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset = load_dataset(\"EdinburghNLP/xsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:47.099742Z","iopub.execute_input":"2025-04-23T23:58:47.100214Z","iopub.status.idle":"2025-04-23T23:58:54.712141Z","shell.execute_reply.started":"2025-04-23T23:58:47.100189Z","shell.execute_reply":"2025-04-23T23:58:54.711346Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b31345ef4464bd6822bc2d031e8f019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e55b2e0c3bea4a8d8cf00c48db70f412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82eda49827d547588f9c0ef8f6b47025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205a0c797b8b465d9a07e97907b6c258"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/17.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a70e5d8fbb45afbfe3d776614c95dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"870f7d5c195945718ccb8db58ff615df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a104d3baec469388c19cae7be281c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a3a8310b064458a9086369c3a4d01a7"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(f\"Dataset loaded: {dataset}\")\nprint(f\"Train set size: {len(dataset['train'])}\")\nprint(f\"Validation set size: {len(dataset['validation'])}\")\nprint(f\"Test set size: {len(dataset['test'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:58:58.499771Z","iopub.execute_input":"2025-04-23T23:58:58.500683Z","iopub.status.idle":"2025-04-23T23:58:58.508698Z","shell.execute_reply.started":"2025-04-23T23:58:58.500642Z","shell.execute_reply":"2025-04-23T23:58:58.506616Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded: DatasetDict({\n    train: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11334\n    })\n})\nTrain set size: 204045\nValidation set size: 11332\nTest set size: 11334\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"sample = dataset[\"train\"][0]\nprint(\"\\nSample document:\")\nprint(sample[\"document\"][:500] + \"...\\n\")\nprint(\"Sample summary:\")\nprint(sample[\"summary\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:00.875561Z","iopub.execute_input":"2025-04-23T23:59:00.875851Z","iopub.status.idle":"2025-04-23T23:59:01.477942Z","shell.execute_reply.started":"2025-04-23T23:59:00.875822Z","shell.execute_reply":"2025-04-23T23:59:01.476827Z"}},"outputs":[{"name":"stdout","text":"\nSample document:\nThe full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\nThe water...\n\nSample summary:\nClean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def analyze_dataset(dataset, split=\"train\", num_samples=1000):\n    \"\"\"Analyze document and summary lengths in the dataset.\"\"\"\n    if num_samples > len(dataset[split]):\n        num_samples = len(dataset[split])\n    \n    doc_lengths = []\n    summary_lengths = []\n    compression_ratios = []\n    \n    for i in range(num_samples):\n        doc = dataset[split][i][\"document\"]\n        summary = dataset[split][i][\"summary\"]\n        \n        doc_words = len(doc.split())\n        summary_words = len(summary.split())\n        \n        doc_lengths.append(doc_words)\n        summary_lengths.append(summary_words)\n        \n        if doc_words > 0:\n            compression_ratios.append(summary_words / doc_words)\n    \n    return {\n        \"doc_lengths\": {\n            \"mean\": np.mean(doc_lengths),\n            \"median\": np.median(doc_lengths),\n            \"min\": np.min(doc_lengths),\n            \"max\": np.max(doc_lengths),\n        },\n        \"summary_lengths\": {\n            \"mean\": np.mean(summary_lengths),\n            \"median\": np.median(summary_lengths),\n            \"min\": np.min(summary_lengths),\n            \"max\": np.max(summary_lengths),\n        },\n        \"compression_ratio\": {\n            \"mean\": np.mean(compression_ratios),\n            \"median\": np.median(compression_ratios),\n        }\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:06.536816Z","iopub.execute_input":"2025-04-23T23:59:06.537610Z","iopub.status.idle":"2025-04-23T23:59:06.543729Z","shell.execute_reply.started":"2025-04-23T23:59:06.537587Z","shell.execute_reply":"2025-04-23T23:59:06.542959Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"analysis = analyze_dataset(dataset)\nprint(\"\\nDataset Analysis:\")\nprint(f\"Document length (words): {analysis['doc_lengths']}\")\nprint(f\"Summary length (words): {analysis['summary_lengths']}\")\nprint(f\"Compression ratio: {analysis['compression_ratio']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:11.493199Z","iopub.execute_input":"2025-04-23T23:59:11.493923Z","iopub.status.idle":"2025-04-23T23:59:11.619941Z","shell.execute_reply.started":"2025-04-23T23:59:11.493899Z","shell.execute_reply":"2025-04-23T23:59:11.619268Z"}},"outputs":[{"name":"stdout","text":"\nDataset Analysis:\nDocument length (words): {'mean': 361.979, 'median': 289.0, 'min': 11, 'max': 2694}\nSummary length (words): {'mean': 21.101, 'median': 21.0, 'min': 1, 'max': 48}\nCompression ratio: {'mean': 0.10330915428072976, 'median': 0.07246376811594203}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"MODEL_NAME = \"t5-base\"  # Options: t5-small, t5-base, t5-large, t5-3b, t5-11b\nMAX_SOURCE_LENGTH = 512 \nMAX_TARGET_LENGTH = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:17.057949Z","iopub.execute_input":"2025-04-23T23:59:17.058584Z","iopub.status.idle":"2025-04-23T23:59:17.061938Z","shell.execute_reply.started":"2025-04-23T23:59:17.058560Z","shell.execute_reply":"2025-04-23T23:59:17.061133Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:23.469235Z","iopub.execute_input":"2025-04-23T23:59:23.469982Z","iopub.status.idle":"2025-04-23T23:59:26.425703Z","shell.execute_reply.started":"2025-04-23T23:59:23.469960Z","shell.execute_reply":"2025-04-23T23:59:26.424879Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfb2b3ed19e47a0a3a39a6890d94895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85d2972442b4ddeb958ac6425487388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7c75d66c9a4be5ba2c3191b7910f51"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(f\"Model: {MODEL_NAME}\")\nprint(f\"Vocabulary size: {tokenizer.vocab_size}\")\nprint(f\"Maximum source length: {MAX_SOURCE_LENGTH}\")\nprint(f\"Maximum target length: {MAX_TARGET_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:30.113017Z","iopub.execute_input":"2025-04-23T23:59:30.113731Z","iopub.status.idle":"2025-04-23T23:59:30.118041Z","shell.execute_reply.started":"2025-04-23T23:59:30.113706Z","shell.execute_reply":"2025-04-23T23:59:30.117336Z"}},"outputs":[{"name":"stdout","text":"Model: t5-base\nVocabulary size: 32000\nMaximum source length: 512\nMaximum target length: 64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"PREFIX = \"summarize: \"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:33.562395Z","iopub.execute_input":"2025-04-23T23:59:33.562945Z","iopub.status.idle":"2025-04-23T23:59:33.566553Z","shell.execute_reply.started":"2025-04-23T23:59:33.562924Z","shell.execute_reply":"2025-04-23T23:59:33.565640Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def preprocess_function(examples):\n    \"\"\"\n    Preprocess the dataset for T5 fine-tuning.\n    T5 was trained with the prefix format, so we add \"summarize: \" before the input text.\n    \"\"\"\n    # Add prefix to the inputs\n    inputs = [PREFIX + doc for doc in examples[\"document\"]]\n    \n    # Tokenize inputs and targets\n    model_inputs = tokenizer(\n        inputs,\n        max_length=MAX_SOURCE_LENGTH,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"summary\"],\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in loss calculation\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:36.604392Z","iopub.execute_input":"2025-04-23T23:59:36.604647Z","iopub.status.idle":"2025-04-23T23:59:36.610122Z","shell.execute_reply.started":"2025-04-23T23:59:36.604632Z","shell.execute_reply":"2025-04-23T23:59:36.609357Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenized_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    desc=\"Preprocessing dataset\",\n)\n\nprint(\"Dataset preprocessing completed.\")\nprint(f\"Columns in processed dataset: {tokenized_datasets['train'].column_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T23:59:40.070751Z","iopub.execute_input":"2025-04-23T23:59:40.071635Z","iopub.status.idle":"2025-04-24T00:08:33.204849Z","shell.execute_reply.started":"2025-04-23T23:59:40.071607Z","shell.execute_reply":"2025-04-24T00:08:33.204250Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Preprocessing dataset:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034eaeb1ae884afbad91cca6c3916ffa"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing dataset:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cd45b7a6c84a9eb5ac846532305935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Preprocessing dataset:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d319fe496a6b45ca9748f888462fff24"}},"metadata":{}},{"name":"stdout","text":"Dataset preprocessing completed.\nColumns in processed dataset: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\nprint(f\"Model loaded: {MODEL_NAME}\")\nprint(f\"Number of parameters: {model.num_parameters():,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:09:39.103190Z","iopub.execute_input":"2025-04-24T00:09:39.103524Z","iopub.status.idle":"2025-04-24T00:10:14.776079Z","shell.execute_reply.started":"2025-04-24T00:09:39.103502Z","shell.execute_reply":"2025-04-24T00:10:14.775336Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048ef4b98f414b2da3c5c8726acd04e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34bf6fe7df0d413bbfaa36b4904e6193"}},"metadata":{}},{"name":"stdout","text":"Model loaded: t5-base\nNumber of parameters: 222,903,552\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:10:18.482471Z","iopub.execute_input":"2025-04-24T00:10:18.482868Z","iopub.status.idle":"2025-04-24T00:10:18.940564Z","shell.execute_reply.started":"2025-04-24T00:10:18.482834Z","shell.execute_reply":"2025-04-24T00:10:18.939768Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"batch_size = 8\ngradient_accumulation_steps = 4\neffective_batch_size = batch_size * gradient_accumulation_steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:10:21.631100Z","iopub.execute_input":"2025-04-24T00:10:21.631694Z","iopub.status.idle":"2025-04-24T00:10:21.635040Z","shell.execute_reply.started":"2025-04-24T00:10:21.631670Z","shell.execute_reply":"2025-04-24T00:10:21.634443Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=f\"./results/{MODEL_NAME}-xsum\",\n    eval_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,  # Mixed precision training\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    generation_max_length=MAX_TARGET_LENGTH,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",\n    greater_is_better=True,\n    warmup_steps=500,\n    report_to=\"none\",  # Set to \"wandb\" if using Weights & Biases\n)\n\nprint(f\"Effective batch size: {effective_batch_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:18.180197Z","iopub.execute_input":"2025-04-24T00:11:18.181078Z","iopub.status.idle":"2025-04-24T00:11:18.209871Z","shell.execute_reply.started":"2025-04-24T00:11:18.181050Z","shell.execute_reply":"2025-04-24T00:11:18.209280Z"}},"outputs":[{"name":"stdout","text":"Effective batch size: 32\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"rouge_metric = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:21.701152Z","iopub.execute_input":"2025-04-24T00:11:21.701916Z","iopub.status.idle":"2025-04-24T00:11:23.453738Z","shell.execute_reply.started":"2025-04-24T00:11:21.701890Z","shell.execute_reply":"2025-04-24T00:11:23.453008Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015ef15855e3424dadfc36b2432d64e0"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n    preds, labels = eval_preds\n    \n    # Decode generated summaries\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Compute ROUGE scores\n    result = rouge_metric.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        use_stemmer=True,\n    )\n    \n    # Extract median scores\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:29.667486Z","iopub.execute_input":"2025-04-24T00:11:29.668082Z","iopub.status.idle":"2025-04-24T00:11:29.674291Z","shell.execute_reply.started":"2025-04-24T00:11:29.668058Z","shell.execute_reply":"2025-04-24T00:11:29.673624Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100,\n    padding=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:35.802777Z","iopub.execute_input":"2025-04-24T00:11:35.803050Z","iopub.status.idle":"2025-04-24T00:11:35.806952Z","shell.execute_reply.started":"2025-04-24T00:11:35.803030Z","shell.execute_reply":"2025-04-24T00:11:35.806155Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:41.062515Z","iopub.execute_input":"2025-04-24T00:11:41.062789Z","iopub.status.idle":"2025-04-24T00:11:41.091220Z","shell.execute_reply.started":"2025-04-24T00:11:41.062770Z","shell.execute_reply":"2025-04-24T00:11:41.090495Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1485828738.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def custom_training_loop(model, tokenizer, train_dataset, val_dataset, num_epochs=3):\n    \"\"\"\n    Custom training loop with linear scheduler with warmup.\n    This demonstrates more control over the training process compared to using Trainer.\n    \"\"\"\n    from torch.utils.data import DataLoader\n    from torch.optim import AdamW\n    \n    # Prepare data loaders\n    train_dataloader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        collate_fn=data_collator\n    )\n    \n    val_dataloader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        collate_fn=data_collator\n    )\n    \n    # Optimizer\n    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n    \n    # Learning rate scheduler\n    total_steps = len(train_dataloader) * num_epochs\n    warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n    \n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps,\n    )\n    \n    # Training loop\n    progress_bar = tqdm(range(total_steps))\n    global_step = 0\n    best_rouge1 = 0\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        \n        for batch in train_dataloader:\n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Scale loss for gradient accumulation\n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n            \n            if (global_step + 1) % gradient_accumulation_steps == 0:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            \n            train_loss += loss.item()\n            global_step += 1\n            progress_bar.update(1)\n            \n            # Log training loss\n            if global_step % 100 == 0:\n                print(f\"Step {global_step}: Loss = {train_loss / (global_step % len(train_dataloader) or 1)}\")\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {avg_train_loss}\")\n        \n        # Evaluation\n        model.eval()\n        val_loss = 0\n        all_preds = []\n        all_labels = []\n        \n        for batch in val_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            with torch.no_grad():\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n                \n                # Generate predictions\n                generated_tokens = model.generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    max_length=MAX_TARGET_LENGTH,\n                    num_beams=4,\n                    length_penalty=0.6,\n                    early_stopping=True,\n                )\n                \n                all_preds.extend(generated_tokens.cpu().numpy())\n                all_labels.extend(batch[\"labels\"].cpu().numpy())\n        \n        # Compute metrics\n        metrics = compute_metrics((np.array(all_preds), np.array(all_labels)))\n        \n        avg_val_loss = val_loss / len(val_dataloader)\n        print(f\"Validation loss: {avg_val_loss}\")\n        print(f\"ROUGE scores: {metrics}\")\n        \n        # Save best model\n        if metrics[\"rouge1\"] > best_rouge1:\n            best_rouge1 = metrics[\"rouge1\"]\n            # Save model checkpoint\n            torch.save(model.state_dict(), f\"./checkpoint_epoch_{epoch+1}.pt\")\n            print(f\"New best model saved with ROUGE-1: {best_rouge1}\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:45.922168Z","iopub.execute_input":"2025-04-24T00:11:45.922873Z","iopub.status.idle":"2025-04-24T00:11:45.933436Z","shell.execute_reply.started":"2025-04-24T00:11:45.922850Z","shell.execute_reply":"2025-04-24T00:11:45.932690Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Uncomment to use custom training loop instead of the Trainer\ncustom_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\ncustom_model = custom_training_loop(\n    custom_model, \n    tokenizer, \n    tokenized_datasets[\"train\"], \n    tokenized_datasets[\"validation\"], \n    num_epochs=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:14:31.467656Z","iopub.execute_input":"2025-04-24T00:14:31.468412Z","execution_failed":"2025-04-24T00:15:13.279Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/76518 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05e927b77914f6f842cab0a40e329c3"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def generate_summary(text, model, tokenizer, max_length=MAX_TARGET_LENGTH):\n    \"\"\"Generate a summary for the given text.\"\"\"\n    # Prepare input\n    input_text = PREFIX + text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_SOURCE_LENGTH, truncation=True).to(device)\n    \n    # Generate\n    outputs = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=max_length,\n        num_beams=4,\n        length_penalty=0.6,\n        early_stopping=True,\n    )\n    \n    # Decode and return the summary\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:56.399085Z","iopub.execute_input":"2025-04-24T00:11:56.399886Z","iopub.status.idle":"2025-04-24T00:11:56.404175Z","shell.execute_reply.started":"2025-04-24T00:11:56.399860Z","shell.execute_reply":"2025-04-24T00:11:56.403430Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"test_samples = dataset[\"test\"].select(range(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:11:58.750236Z","iopub.execute_input":"2025-04-24T00:11:58.750813Z","iopub.status.idle":"2025-04-24T00:11:58.756877Z","shell.execute_reply.started":"2025-04-24T00:11:58.750791Z","shell.execute_reply":"2025-04-24T00:11:58.756110Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"for i, sample in enumerate(test_samples):\n    document = sample[\"document\"]\n    reference_summary = sample[\"summary\"]\n    \n    # Generate summary\n    generated_summary = generate_summary(document, model, tokenizer)\n    \n    print(f\"\\nExample {i+1}:\")\n    print(f\"Document (truncated): {document[:200]}...\")\n    print(f\"Reference summary: {reference_summary}\")\n    print(f\"Generated summary: {generated_summary}\")\n    \n    # Calculate ROUGE for individual example\n    rouge_result = rouge_metric.compute(\n        predictions=[generated_summary],\n        references=[reference_summary],\n        use_stemmer=True,\n    )\n    \n    print(f\"ROUGE scores: {rouge_result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:12:11.126817Z","iopub.execute_input":"2025-04-24T00:12:11.127523Z","iopub.status.idle":"2025-04-24T00:12:17.251824Z","shell.execute_reply.started":"2025-04-24T00:12:11.127491Z","shell.execute_reply":"2025-04-24T00:12:17.251123Z"}},"outputs":[{"name":"stdout","text":"\nExample 1:\nDocument (truncated): Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\nWorkers at the charity claim investment in housing...\nReference summary: There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\nGenerated summary: prison link cymru says some ex-offenders are living rough for up to a year . charity says investment in housing would be cheaper than jailing homeless repeat offenders . the government says more people than ever are getting help to address housing problems .\nROUGE scores: {'rouge1': 0.27118644067796605, 'rouge2': 0.03508771929824562, 'rougeL': 0.13559322033898302, 'rougeLsum': 0.13559322033898302}\n\nExample 2:\nDocument (truncated): Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\nDetectives said three firearms, ammunition and a five-figure sum of money were recovered.\nA 26-yea...\nReference summary: A man has appeared in court after firearms, ammunition and cash were seized by police in Edinburgh.\nGenerated summary: three firearms, ammunition and a five-figure sum of money were recovered . a 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court .\nROUGE scores: {'rouge1': 0.409090909090909, 'rouge2': 0.09523809523809525, 'rougeL': 0.22727272727272727, 'rougeLsum': 0.22727272727272727}\n\nExample 3:\nDocument (truncated): Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Covington, 24, appeared in a Chicago court on Friday.\nThe four have been charged with hate crimes and aggravated kidnapping and...\nReference summary: Four people accused of kidnapping and torturing a mentally disabled man in a \"racially motivated\" attack streamed on Facebook have been denied bail.\nGenerated summary: four charged with hate crimes and aggravated kidnapping and battery . online fundraiser for victim has collected $51,000 (£42,500) so far . judge asks: \"where was your sense of decency?\"\nROUGE scores: {'rouge1': 0.15094339622641512, 'rouge2': 0.0392156862745098, 'rougeL': 0.11320754716981132, 'rougeLsum': 0.11320754716981132}\n\nExample 4:\nDocument (truncated): The 48-year-old former Arsenal goalkeeper played for the Royals for four years.\nHe was appointed youth academy director in 2000 and has been director of football since 2003.\nA West Brom statement said...\nReference summary: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\nGenerated summary: former arsenal goalkeeper has been director of football since 2003 . he was appointed youth academy director in 2000 .\nROUGE scores: {'rouge1': 0.11764705882352941, 'rouge2': 0.0, 'rougeL': 0.11764705882352941, 'rougeLsum': 0.11764705882352941}\n\nExample 5:\nDocument (truncated): Restoring the function of the organ - which helps control blood sugar levels - reversed symptoms of diabetes in animal experiments.\nThe study, published in the journal Cell, says the diet reboots the ...\nReference summary: The pancreas can be triggered to regenerate itself through a type of fasting diet, say US researchers.\nGenerated summary: mice put on a modified form of the \"fasting-mimicking diet\" they spend five days on a low calorie, low protein, low carbohydrate but high unsaturated-fat diet . they then have 25 days eating what they want - so overall it mimics\nROUGE scores: {'rouge1': 0.17241379310344826, 'rouge2': 0.0, 'rougeL': 0.13793103448275862, 'rougeLsum': 0.13793103448275862}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def evaluate_on_test_set(model, test_dataset, tokenizer, batch_size=4):\n    \"\"\"Evaluate the model on the full test set.\"\"\"\n    # Preprocess test set\n    preprocessed_test = test_dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=test_dataset.column_names,\n        desc=\"Preprocessing test set\",\n    )\n    \n    # Create test dataloader\n    test_dataloader = torch.utils.data.DataLoader(\n        preprocessed_test,\n        batch_size=batch_size,\n        collate_fn=data_collator,\n    )\n    \n    # Generate summaries for entire test set\n    model.eval()\n    all_generated_summaries = []\n    all_reference_summaries = []\n    \n    # Original test data for reference summaries\n    original_test_data = dataset[\"test\"]\n    \n    for i, batch in enumerate(tqdm(test_dataloader, desc=\"Generating summaries\")):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        with torch.no_grad():\n            generated_tokens = model.generate(\n                batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=MAX_TARGET_LENGTH,\n                num_beams=4,\n                length_penalty=0.6,\n                early_stopping=True,\n            )\n            \n            # Decode generated summaries\n            decoded_summaries = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            all_generated_summaries.extend(decoded_summaries)\n            \n            # Get reference summaries from original dataset\n            start_idx = i * batch_size\n            end_idx = min(start_idx + batch_size, len(original_test_data))\n            reference_summaries = [original_test_data[j][\"summary\"] for j in range(start_idx, end_idx)]\n            all_reference_summaries.extend(reference_summaries)\n    \n    # Compute ROUGE scores\n    rouge_results = rouge_metric.compute(\n        predictions=all_generated_summaries,\n        references=all_reference_summaries,\n        use_stemmer=True,\n    )\n    \n    # Format results for display\n    formatted_results = {k: round(v * 100, 2) for k, v in rouge_results.items()}\n    return formatted_results, all_generated_summaries, all_reference_summaries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:12:23.318098Z","iopub.execute_input":"2025-04-24T00:12:23.318413Z","iopub.status.idle":"2025-04-24T00:12:23.325768Z","shell.execute_reply.started":"2025-04-24T00:12:23.318391Z","shell.execute_reply":"2025-04-24T00:12:23.325015Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(\"Evaluating on test set...\")\ntest_results, generated_summaries, reference_summaries = evaluate_on_test_set(\n    model, dataset[\"test\"], tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:12:37.826335Z","iopub.execute_input":"2025-04-24T00:12:37.826633Z","iopub.status.idle":"2025-04-24T00:14:08.978018Z","shell.execute_reply.started":"2025-04-24T00:12:37.826612Z","shell.execute_reply":"2025-04-24T00:14:08.976979Z"}},"outputs":[{"name":"stdout","text":"Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating summaries:   0%|          | 0/2834 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c967e1aaa546a28eddb944dbafa5fc"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3829877016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating on test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m test_results, generated_summaries, reference_summaries = evaluate_on_test_set(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_31/1224284900.py\u001b[0m in \u001b[0;36mevaluate_on_test_set\u001b[0;34m(model, test_dataset, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             generated_tokens = model.generate(\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m             )\n\u001b[1;32m   2481\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2483\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3900\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3902\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3904\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1906\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 )\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1132\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     ):\n\u001b[0;32m--> 682\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    598\u001b[0m     ):\n\u001b[1;32m    599\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"print(\"Test set evaluation results:\")\nprint(test_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_generation_strategies(text, model, tokenizer, max_length=MAX_TARGET_LENGTH):\n    \"\"\"Compare different generation strategies for the given text.\"\"\"\n    input_text = PREFIX + text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_SOURCE_LENGTH, truncation=True).to(device)\n    \n    # Strategy 1: Standard Beam Search\n    outputs_beam = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=max_length,\n        num_beams=4,\n        early_stopping=True,\n    )\n    \n    # Strategy 2: Beam Search with Length Penalty\n    outputs_length_penalty = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=max_length,\n        num_beams=4,\n        length_penalty=0.6,  # < 1.0 favors shorter sequences\n        early_stopping=True,\n    )\n    \n    # Strategy 3: Diverse Beam Search\n    outputs_diverse_beam = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=max_length,\n        num_beams=4,\n        num_beam_groups=4,\n        diversity_penalty=0.5,  # Promotes diversity between groups\n        early_stopping=True,\n    )\n    \n    # Strategy 4: Top-p (Nucleus) Sampling\n    outputs_top_p = model.generate(\n        inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=max_length,\n        do_sample=True,\n        top_p=0.92,\n        top_k=0,\n        temperature=0.7,\n    )\n    \n    # Decode\n    summary_beam = tokenizer.decode(outputs_beam[0], skip_special_tokens=True)\n    summary_length_penalty = tokenizer.decode(outputs_length_penalty[0], skip_special_tokens=True)\n    summary_diverse_beam = tokenizer.decode(outputs_diverse_beam[0], skip_special_tokens=True)\n    summary_top_p = tokenizer.decode(outputs_top_p[0], skip_special_tokens=True)\n    \n    return {\n        \"standard_beam\": summary_beam,\n        \"length_penalty\": summary_length_penalty,\n        \"diverse_beam\": summary_diverse_beam,\n        \"top_p_sampling\": summary_top_p\n    }","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_doc = dataset[\"test\"][10][\"document\"]\nreference = dataset[\"test\"][10][\"summary\"]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nComparing different generation strategies:\")\nprint(f\"Document (truncated): {sample_doc[:200]}...\")\nprint(f\"Reference summary: {reference}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generation_results = compare_generation_strategies(sample_doc, model, tokenizer)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for strategy, summary in generation_results.items():\n    print(f\"\\n{strategy.upper()} strategy:\")\n    print(summary)\n    \n    # Calculate ROUGE for individual strategy\n    rouge_result = rouge_metric.compute(\n        predictions=[summary],\n        references=[reference],\n        use_stemmer=True,\n    )\n    \n    formatted_rouge = {k: round(v * 100, 2) for k, v in rouge_result.items()}\n    print(f\"ROUGE scores: {formatted_rouge}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_task_prefixes(text, model, tokenizer, max_length=MAX_TARGET_LENGTH):\n    \"\"\"Compare different task prefixes for T5 summarization.\"\"\"\n    prefixes = [\n        \"summarize: \",\n        \"generate summary: \",\n        \"tl;dr: \",\n        \"summarization: \",\n    ]\n    \n    results = {}\n    \n    for prefix in prefixes:\n        input_text = prefix + text\n        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_SOURCE_LENGTH, truncation=True).to(device)\n        \n        outputs = model.generate(\n            inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_length=max_length,\n            num_beams=4,\n            length_penalty=0.6,\n            early_stopping=True,\n        )\n        \n        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        results[prefix] = summary\n    \n    return results","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nComparing different T5 task prefixes:\")\nprefix_results = compare_task_prefixes(sample_doc, model, tokenizer)\n\nfor prefix, summary in prefix_results.items():\n    print(f\"\\nPrefix: '{prefix}'\")\n    print(f\"Generated summary: {summary}\")\n    \n    # Calculate ROUGE for individual prefix\n    rouge_result = rouge_metric.compute(\n        predictions=[summary],\n        references=[reference],\n        use_stemmer=True,\n    )\n    \n    formatted_rouge = {k: round(v * 100, 2) for k, v in rouge_result.items()}\n    print(f\"ROUGE scores: {formatted_rouge}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p ./model_export","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./model_export/tokenizer\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"./model_export/model\"\nmodel.save_pretrained(model_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T18:45:59.729Z"}},"outputs":[],"execution_count":null}]}